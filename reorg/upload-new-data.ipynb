{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: remove unused functions from library / combine/separate libraries into better modules\n",
    "# TODO: put zip file somewhere safe after its been used. \n",
    "# TODO: create regular backups to google docs or S3\n",
    "# TODO: profile code to reduce upload times (strongly suspect df.apply() statements are bad. As in SID creation)\n",
    "# TODO: add plate ID - one for each row in the provided metadata files. Should just be a randomly generated uuid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import shutil\n",
    "import arrow\n",
    "import hashlib\n",
    "import pandas as pd\n",
    "import IPython.html.widgets as widgets\n",
    "\n",
    "from collections import \\\n",
    "    OrderedDict\n",
    "\n",
    "from numpy.random import \\\n",
    "    random\n",
    "\n",
    "from toolz import \\\n",
    "    partition,\\\n",
    "    partitionby,\\\n",
    "    thread_last,\\\n",
    "    thread_first\n",
    "    \n",
    "from utils import \\\n",
    "    snd,\\\n",
    "    exists_at_path,\\\n",
    "    get_layout_data,\\\n",
    "    add_dict_to_dataframe,\\\n",
    "    add_col,\\\n",
    "    maprows,\\\n",
    "    format_num,\\\n",
    "    from_file,\\\n",
    "    format_timestamp,\\\n",
    "    parse_label_group,\\\n",
    "    string_only_contains,\\\n",
    "    generate_sid\n",
    "    \n",
    "from IPython.display import \\\n",
    "    clear_output\n",
    "\n",
    "from raw import \\\n",
    "    get_plate_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# String -> String\n",
    "def rename_column(col):\n",
    "    \"\"\" Rename column col to remove whitespace, backslashes, prefixes,\n",
    "        and suffixes (esp. large parenthetic suffix). \"\"\"\n",
    "    if col.startswith('Cell:'):\n",
    "        return col.split('(')[0].lstrip(\"Cell:\").rstrip('/').strip(' ')\n",
    "    else:\n",
    "        return col.split('(')[0].rstrip('/').strip(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "normalization_config = \\\n",
    "   [['Normalized_ColocSpot_area_sum (coloc)',\n",
    "      ['ColocSpots_area_sum'],\n",
    "      ['FITC-TxRed_coloc_area_sum']],\n",
    "    ['Normalized_ColocSpot_area_sum (all)',\n",
    "      ['ColocSpots_area_sum'],\n",
    "      ['FITC-TxRed_all_area_sum']],\n",
    "\n",
    "    ['Normalized coloc spots (by FITC & TxRed)',\n",
    "      ['# of Coloc Spots'],\n",
    "      ['# of FITC spots', '# of TxRed spots']],\n",
    "    ['Normalized coloc spots (by FITC)',\n",
    "      ['# of Coloc Spots'],\n",
    "      ['# of FITC spots']],\n",
    "    ['Normalized coloc spots (by TxRed)',\n",
    "      ['# of Coloc Spots'],\n",
    "      ['# of TxRed spots']],\n",
    "\n",
    "    ['Normalized coloc spots (by FITC in coloc)',\n",
    "      ['# of Coloc Spots'],\n",
    "      ['# of FITC in ColocSpots']],\n",
    "    ['Normalized coloc spots (by TxRed in coloc)',\n",
    "      ['# of Coloc Spots'],\n",
    "      ['# of TxRed in ColocSpots']],\n",
    "    ['Normalized coloc spots (by FITC-TxRed in coloc)',\n",
    "      ['# of Coloc Spots'],\n",
    "      ['# of FITC-TxRed in ColocSpots']]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plate_import_config = dict(\n",
    "    delimiter = '\\t',\n",
    "    skiprows = 4,\n",
    "    dropcols = ['Laser focus score',\n",
    "                '\\.[0-9]*\\Z'],\n",
    "    colrename = rename_column,\n",
    "    normcols = normalization_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extract files into temporary working directory\n",
    "zipfile_path = '/notebooks/add-data/data.zip'\n",
    "extract_path = '/notebooks/tmp/extracted-data/'\n",
    "temp_save_path = '/notebooks/tmp/imported-data/'\n",
    "db_path = '/notebooks/moldev-data/db/db.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# String -> String\n",
    "def computeMD5hash(string):\n",
    "    m = hashlib.md5()\n",
    "    m.update(string.encode('utf-8'))\n",
    "    return m.hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Series -> String\n",
    "def generate_cell_sid(cell_data):\n",
    "    \"\"\" Given Series containing cell information, \n",
    "        generate hash string to use as string id. \"\"\"\n",
    "    \n",
    "    columns_to_hash = \\\n",
    "        ['Plate ID',\n",
    "         'Well Name',\n",
    "         'Site ID',\n",
    "         'Cell ID']\n",
    "    \n",
    "    return thread_last(\n",
    "        cell_data[columns_to_hash].tolist(),\n",
    "        (map,str),\n",
    "        (str.join,' '),\n",
    "        computeMD5hash, \n",
    "        lambda string: 'CELL_' + string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Series -> DataFrame\n",
    "def gather_plate_data(plate_metadata):\n",
    "    \"\"\" Given Series containing filepaths for plate and layout,\n",
    "        import these files, join them, and add the series itself \n",
    "        to create a master table for all the info about the plate. \"\"\"\n",
    "    \n",
    "    # String -> String -> String\n",
    "    def get_path(directory,column):\n",
    "        \"\"\" Return path with first folder at given directory, \n",
    "            and file at given column of metadata.csv file. \n",
    "            \n",
    "            (i.e. go to folder X and get file found in column Y of metadata file.)\"\"\"\n",
    "            \n",
    "        return os.path.join(\n",
    "            extract_path,\n",
    "            directory,\n",
    "            plate_metadata[column])\n",
    "        \n",
    "    plate_data = thread_last(\n",
    "        ['Plates','Plate File'],\n",
    "        (apply,get_path),\n",
    "        lambda path: get_plate_data(path,\n",
    "                                    plate_import_config))\n",
    "    \n",
    "    # Add string ID for use as primary key\n",
    "    plate_data['Cell SID'] = plate_data.apply(generate_cell_sid,\n",
    "                                              axis = 1)\n",
    "    \n",
    "#     plate_data['Plate SID'] = \"Plate_{}\".format(generate_sid())\n",
    "    \n",
    "    layout_data = thread_last(\n",
    "        ['Layouts','Layout File'],\n",
    "        (apply,get_path),\n",
    "        lambda path: get_layout_data(path))\n",
    "    \n",
    "    return thread_first(\n",
    "        pd.merge(plate_data,layout_data,on = 'Well Name'),\n",
    "        (add_dict_to_dataframe,dict(plate_metadata)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DataFrame -> DataFrame -> [Timestamp]\n",
    "def find_uploads_with_duplicate_cells(db_dataframe,new_dataframe):\n",
    "    \"\"\" Given a primary dataframe acting as central information store, \n",
    "        and a new dataframe containing data to be incorporated into the primary store, \n",
    "        check if there are any duplicated cells, and when they were added. \n",
    "        \n",
    "        Returns list of timestamps for days when duplicate cells were uploaded. \n",
    "        (Returns empty list if there are no duplicate cells.) \"\"\"\n",
    "    \n",
    "    new_cell_sids = new_dataframe['Cell SID']\n",
    "    duplicate_cells = db_dataframe['Cell SID'].isin(new_cell_sids)\n",
    "    return db_dataframe[duplicate_cells]['Upload Timestamp'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_message(tests):\n",
    "    \"\"\" Print out statements for all tests that fail. \"\"\"\n",
    "    return thread_last(\n",
    "        tests,\n",
    "        (partition,2),\n",
    "        (filter,lambda pair: pair[0] == False),\n",
    "        (map,snd),\n",
    "        (str.join,'\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check(_):\n",
    "    \"\"\" Extract zip and prepare for import into main dataset. \n",
    "        If data can be imported, then create a new csv in a temp directory. \n",
    "        Returns string of any warning or errors in this process. \"\"\"\n",
    "    \n",
    "    if os.path.exists(extract_path):\n",
    "        shutil.rmtree(extract_path) # clear out existing files\n",
    "\n",
    "    with zipfile.ZipFile(zipfile_path, \"r\") as z:\n",
    "        z.extractall(extract_path)    \n",
    "        \n",
    "    # Check files for correctness\n",
    "    exists = exists_at_path(extract_path) # curried function\n",
    "    nonempty = lambda entity: len(os.listdir(os.path.join(extract_path,entity))) > 0\n",
    "    initial_tests = \\\n",
    "        [exists('metadata.csv'), \"File missing: metadata.csv\",\n",
    "         exists('Plates/'), \"Folder missing: Plates\",\n",
    "         exists('Layouts/'), \"Folder missing: Layouts\",\n",
    "         nonempty('Plates/'), \"It looks like you haven't got any plates in your Plates folder.\",\n",
    "         nonempty('Layouts/'), \"It looks like you haven't got any layouts in your Layouts folder.\"]\n",
    "    \n",
    "    err = generate_message(initial_tests)\n",
    "    clear_output()\n",
    "    \n",
    "    if err != '':\n",
    "        print \"### ERROR ###\"\n",
    "        print err\n",
    "    else: \n",
    "        # Read metadata\n",
    "        metadata_path = os.path.join(extract_path,'metadata.csv')\n",
    "        metadata = pd.read_csv(metadata_path).dropna(how='all',axis=0).dropna(how='all',axis=1)\n",
    "        \n",
    "        # Get all data\n",
    "        all_data = thread_last(\n",
    "            metadata,\n",
    "            (maprows,gather_plate_data),\n",
    "            pd.concat)\n",
    "        \n",
    "        all_data['Upload Timestamp'] = arrow.now().timestamp\n",
    "        \n",
    "        all_data['Condition'] = \\\n",
    "            all_data['Assay'] + ' ' + \\\n",
    "            all_data['Concentration'].map(str) + ' ' + \\\n",
    "            all_data['Units (concentration)'] + ' ' + \\\n",
    "            all_data['Contents'] + ' ' + \\\n",
    "            all_data['Cell Type']\n",
    "        \n",
    "        # Check for duplicated cells\n",
    "        db_dataframe = pd.read_csv(db_path)\n",
    "        duplicate_timestamps = find_uploads_with_duplicate_cells(db_dataframe,all_data)\n",
    "        if len(duplicate_timestamps) > 0:\n",
    "            for ts in duplicate_timestamps:\n",
    "                time = arrow.get(ts).to('US/Pacific').format('MMMM DD, YYYY, h:mm a')\n",
    "                time_ago = arrow.get(ts).humanize()\n",
    "                print \"It looks like you already uploaded some of this data on {} ({})\".format(time,time_ago)\n",
    "            print \"If you'd like to overwrite this data, you'll need to remove the data for these dates first.\"\n",
    "        else: \n",
    "            print \"Ready to upload {} cells!\".format(format_num(len(all_data)))\n",
    "            \n",
    "        # Save data to temporary location\n",
    "        if os.path.exists(temp_save_path):\n",
    "            shutil.rmtree(temp_save_path)\n",
    "        os.makedirs(temp_save_path)\n",
    "        all_data.to_csv(os.path.join(temp_save_path,'new_data.csv'),\n",
    "                        index=False)\n",
    "    return\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stage_button = widgets.Button(description = \"Check data\")\n",
    "stage_button.on_click(check)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare and check data\n",
    "This step takes around 3 minutes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stage_button"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_new_data(_):\n",
    "    \"\"\" Add new data (from temp file) to db file.\n",
    "        Fails if there are duplicate cells.\"\"\"\n",
    "    db_data = pd.read_csv(db_path)\n",
    "    new_data = pd.read_csv(os.path.join(temp_save_path,'new_data.csv'))\n",
    "    all_data = pd.concat([db_data,new_data])\n",
    "    contains_duplicated_cells = all_data.duplicated('Cell SID').any()\n",
    "    \n",
    "    clear_output()\n",
    "    \n",
    "    if contains_duplicated_cells:\n",
    "        print \"It looks like the data's already been added.\"\n",
    "    else:\n",
    "        all_data.to_csv(db_path,index = False)\n",
    "        print \"Just saved data!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save data\n",
    "Once the data's been checked for correctness (all files are present, and none of the data has already been uploaded), save it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "save_button = widgets.Button(description = \"Save data\",background_color='Green',color = 'white')\n",
    "save_button.on_click(add_new_data)\n",
    "save_button"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delete past uploads\n",
    "If you've made any mistake, and need to delete something you've uploaded, this is the place to do it. Just select the upload that you'd like to remove, and click `delete`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Just deleted data.\n"
     ]
    }
   ],
   "source": [
    "db_data = pd.read_csv(db_path)\n",
    "timestamps = db_data['Upload Timestamp'].unique()\n",
    "\n",
    "delete_options = thread_last(\n",
    "    timestamps,\n",
    "    list,\n",
    "    lambda x: sorted(x,reverse=True),\n",
    "    (map,lambda x: (x,x)),\n",
    "    (map,lambda x: (format_timestamp(x[0]),x[1])),\n",
    "    OrderedDict)\n",
    "\n",
    "delete_dropdown = widgets.Dropdown(options = delete_options)\n",
    "\n",
    "def delete_handler(_):\n",
    "    \"\"\" Remove data uploaded at selected timestamp. \"\"\"\n",
    "    timestamp = delete_dropdown.value\n",
    "    trimmed_data = db_data[db_data['Upload Timestamp'] != timestamp]\n",
    "    trimmed_data.to_csv(db_path,index=False)\n",
    "    clear_output()\n",
    "    print \"Just deleted data.\"\n",
    "\n",
    "delete_button = widgets.Button(description = 'Delete',background_color='Red',color = 'white')\n",
    "delete_button.on_click(delete_handler)\n",
    "widgets.HBox(children = [delete_button,delete_dropdown])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# testpath = '/notebooks/tmp/extracted-data/Plates/APB HS JS (60X) 08.06.2015 siRNA VE821.txt'\n",
    "# test = get_plate_data(testpath,plate_import_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# layouttest = get_layout_data('/notebooks/tmp/extracted-data/Layouts/layout.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test['Well Name'].unique()\n",
    "# test2 = pd.read_csv('/notebooks/tmp/imported-data/new_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # String -> [String]\n",
    "# def split_on_newlines(string):\n",
    "#     \"\"\" Given a string which may contain \\r, \\n, or both, \n",
    "#         split on newlines so neither character is present in output. \"\"\"\n",
    "    \n",
    "#     r = '\\r' in string\n",
    "#     n = '\\n' in string\n",
    "    \n",
    "#     if r and n: \n",
    "#         return string.replace('\\r','').split('\\n')\n",
    "#     elif r:\n",
    "#         return string.split('\\r')\n",
    "#     else:\n",
    "#         return string.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# l2 = thread_last(\n",
    "#     '/notebooks/tmp/extracted-data/Layouts/layout.csv',\n",
    "#     from_file,\n",
    "#     lambda string: string.replace('\\r','').split('\\n'),\n",
    "#     (map,lambda line: line.rstrip(','))  \n",
    "#     (partitionby, lambda line: string_only_contains(line,',')),\n",
    "#     (filter,lambda group: not string_only_contains(group[0],',')),\n",
    "#     (map,lambda strings: str.join('\\n',strings)),\n",
    "#     (map,parse_label_group),\n",
    "#     (reduce,lambda left,right: pd.merge(left,right,on='Well Name')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # String -> Boolean\n",
    "# def string_is_empty(string):\n",
    "#     \"\"\" Return True if string is empty. \"\"\"\n",
    "#     return string == ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# l2 = thread_last(\n",
    "#     '/notebooks/tmp/extracted-data/Layouts/layout.csv',\n",
    "#     from_file,\n",
    "#     split_on_newlines,\n",
    "#     (map,lambda line: line.rstrip(',')),\n",
    "#     (partitionby, string_is_empty),\n",
    "#     (filter,lambda group: not string_is_empty(group[0])),\n",
    "#     (map,lambda strings: str.join('\\n',strings)),\n",
    "#     (map,parse_label_group),\n",
    "#     (reduce,lambda left,right: pd.merge(left,right,on='Well Name')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for x in l2['Units (concentration)'].unique():\n",
    "#     print x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# list(l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
